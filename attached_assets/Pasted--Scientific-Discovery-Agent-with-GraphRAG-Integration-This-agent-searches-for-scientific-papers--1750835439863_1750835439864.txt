"""
Scientific Discovery Agent with GraphRAG Integration

This agent searches for scientific papers and code repositories based on user interests,
extracts key information, and builds a knowledge graph of related concepts and entities.

Author: Assistant
Dependencies: langchain, neo4j, firecrawl, arxiv, biopython, requests
"""

import os
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path
from dataclasses import dataclass
import time
from functools import wraps
from types import SimpleNamespace

import arxiv
import requests
from Bio import Entrez
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import BaseTool, tool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document
from pydantic import BaseModel, Field
from firecrawl import FirecrawlApp
import json
from youtube_transcript_api import YouTubeTranscriptApi
from youtubesearchpython import VideosSearch
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain.prompts import PromptTemplate
from youtube_rag_agent import ask_over_topic
from dotenv import load_dotenv, find_dotenv

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load env variables early so they are available during class initializations
load_dotenv(find_dotenv())

@dataclass
class ResearchPaper:
    """Data structure for research papers"""
    title: str
    authors: List[str]
    abstract: str
    url: str
    publication_date: str
    source: str  # arxiv, bioarxiv, pubmed, github
    keywords: List[str]
    doi: Optional[str] = None
    score: float = 0.0

@dataclass
class GitHubRepo:
    """Data structure for GitHub repositories"""
    name: str
    description: str
    url: str
    language: str
    stars: int
    last_updated: str
    topics: List[str]

class ScientificSearchTools:
    """Collection of tools for searching scientific databases"""
    
    def __init__(self, email: str = "user@example.com"):
        self.email = email
        Entrez.email = email
        
        # Make Firecrawl optional
        firecrawl_key = os.getenv('FIRECRAWL_API_KEY')
        if firecrawl_key:
            try:
                self.firecrawl = FirecrawlApp(api_key=firecrawl_key)
                self.firecrawl_enabled = True
                logger.info("✅ Firecrawl enabled")
            except Exception as e:
                logger.warning(f"⚠️ Firecrawl initialization failed: {e}")
                self.firecrawl = None
                self.firecrawl_enabled = False
        else:
            logger.info("ℹ️ Firecrawl API key not provided, enhanced scraping disabled")
            self.firecrawl = None
            self.firecrawl_enabled = False
        
    def search_arxiv(self, query: str, max_results: int = 10) -> List[Dict]:
        """Search arXiv for papers related to the query"""
        try:
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            papers = []
            for result in search.results():
                paper = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "abstract": result.summary,
                    "url": result.entry_id,
                    "publication_date": result.published.strftime('%Y-%m-%d'),
                    "source": "arxiv",
                    "keywords": [cat for cat in result.categories],
                    "doi": result.doi
                }
                papers.append(paper)
            
            logger.info(f"Found {len(papers)} papers on arXiv for query: {query}")
            return papers
            
        except Exception as e:
            logger.error(f"Error searching arXiv: {e}")
            return []
    
    def search_pubmed(self, query: str, max_results: int = 10) -> List[Dict]:
        """Search PubMed for biomedical papers"""
        try:
            # Search PubMed
            handle = Entrez.esearch(
                db="pubmed", 
                term=query, 
                retmax=max_results,
                sort="pub_date"
            )
            search_results = Entrez.read(handle)
            handle.close()
            
            if not search_results["IdList"]:
                return []
            
            # Fetch paper details
            ids = search_results["IdList"]
            handle = Entrez.efetch(db="pubmed", id=ids, rettype="medline", retmode="text")
            papers_data = handle.read()
            handle.close()
            
            # Parse results (simplified parsing)
            papers = []
            entries = papers_data.split('\n\n')
            
            for entry in entries[:max_results]:
                if 'TI  -' in entry:  # Title indicator
                    lines = entry.split('\n')
                    title = ""
                    authors = []
                    abstract = ""
                    date = ""
                    
                    for line in lines:
                        if line.startswith('TI  -'):
                            title = line[6:].strip()
                        elif line.startswith('AU  -'):
                            authors.append(line[6:].strip())
                        elif line.startswith('AB  -'):
                            abstract = line[6:].strip()
                        elif line.startswith('DP  -'):
                            date = line[6:].strip()
                    
                    if title:
                        paper = {
                            "title": title,
                            "authors": authors,
                            "abstract": abstract,
                            "url": f"https://pubmed.ncbi.nlm.nih.gov/{ids[len(papers)]}/",
                            "publication_date": date,
                            "source": "pubmed",
                            "keywords": [],
                            "doi": None
                        }
                        papers.append(paper)
            
            logger.info(f"Found {len(papers)} papers on PubMed for query: {query}")
            return papers
            
        except Exception as e:
            logger.error(f"Error searching PubMed: {e}")
            return []
    
    def search_bioarxiv(self, query: str, max_results: int = 10) -> List[Dict]:
        """Search bioRxiv for preprints"""
        try:
            # bioRxiv API endpoint
            url = "https://api.biorxiv.org/details/biorxiv"
            
            # Search recent papers (last 30 days)
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            
            response = requests.get(f"{url}/{start_date}/{end_date}")
            
            if response.status_code == 200:
                data = response.json()
                papers = []
                
                for paper_data in data.get('collection', [])[:max_results]:
                    # Filter by query in title or abstract
                    if (query.lower() in paper_data.get('title', '').lower() or 
                        query.lower() in paper_data.get('abstract', '').lower()):
                        
                        paper = {
                            "title": paper_data.get('title', ''),
                            "authors": paper_data.get('authors', '').split(';'),
                            "abstract": paper_data.get('abstract', ''),
                            "url": f"https://www.biorxiv.org/content/{paper_data.get('doi', '')}v1",
                            "publication_date": paper_data.get('date', ''),
                            "source": "bioarxiv",
                            "keywords": paper_data.get('category', '').split(','),
                            "doi": paper_data.get('doi', '')
                        }
                        papers.append(paper)
                
                logger.info(f"Found {len(papers)} papers on bioRxiv for query: {query}")
                return papers
            
            return []
            
        except Exception as e:
            logger.error(f"Error searching bioRxiv: {e}")
            return []
    
    def search_github_repos(self, query: str, max_results: int = 10) -> List[Dict]:
        """Search GitHub for relevant code repositories"""
        try:
            token = os.getenv("GITHUB_TOKEN")
            headers = {'Accept': 'application/vnd.github.v3+json'}
            if token:
                headers['Authorization'] = f'token {token}'
            else:
                logger.warning("GITHUB_TOKEN not set – using unauthenticated GitHub requests (low rate-limit)")
            
            url = "https://api.github.com/search/repositories"
            params = {
                'q': f"{query} language:python",
                'sort': 'stars',
                'order': 'desc',
                'per_page': max_results
            }
            
            logger.info(f"Searching GitHub with query: {query}")
            response = requests.get(url, headers=headers, params=params)
            
            if response.status_code == 200:
                data = response.json()
                repos = data.get('items', [])
                
                formatted_repos = []
                for repo in repos:
                    formatted_repos.append({
                        'name': repo['full_name'],
                        'description': repo['description'],
                        'url': repo['html_url'],
                        'language': repo.get('language'),
                        'stars': repo.get('stargazers_count'),
                        'updated_at': repo.get('updated_at'),
                        'topics': repo.get('topics', [])
                    })
                logger.info(f"Found {len(formatted_repos)} repositories on GitHub for query: {query}")
                return formatted_repos
            else:
                logger.error(f"GitHub API Error: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            logger.error(f"Error searching GitHub: {e}")
            return []

    def search_youtube_transcripts(self, query: str, max_results: int = 10) -> List[Dict]:
        """Search YouTube for videos and retrieve their transcripts."""
        logger.info(f"Searching YouTube for query: {query}")

        api_key = os.getenv("YOUTUBE_API_KEY")
        videos: List[Dict] = []
        try:
            if api_key:
                # Use official YouTube Data API v3 – no third-party libs, avoids proxy issues
                url = (
                    "https://www.googleapis.com/youtube/v3/search"
                    f"?part=snippet&type=video&maxResults={max_results}&q={requests.utils.quote(query)}&key={api_key}"
                )
                resp = requests.get(url, timeout=30)
                resp.raise_for_status()
                data = resp.json()
                for item in data.get("items", []):
                    vid = item["id"]["videoId"]
                    snippet = item["snippet"]
                    videos.append({
                        "id": vid,
                        "title": snippet.get("title"),
                        "channel": snippet.get("channelTitle"),
                        "url": f"https://www.youtube.com/watch?v={vid}",
                        "publishedDate": snippet.get("publishedAt", "")[:10]
                    })
            else:
                logger.warning("YOUTUBE_API_KEY not set – skipping YouTube search (provide key to enable video results)")

            # Retrieve transcripts
            transcripts: List[Dict] = []
            for v in videos:
                try:
                    transcript_text = " ".join(seg["text"] for seg in YouTubeTranscriptApi.get_transcript(v["id"]))
                    v["transcript"] = transcript_text[:2000]
                    transcripts.append(v)
                except Exception as e:
                    logger.debug(f"Transcript fetch failed for {v['id']}: {e}")
                    continue

            logger.info(f"Found {len(transcripts)} YouTube video transcripts for query: {query}")
            return transcripts
        except Exception as e:
            logger.error(f"Error searching YouTube: {e}")
            return []

class KnowledgeGraphBuilder:
    """Builds and manages the knowledge graph for scientific discoveries"""
    
    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        self.graph = None
        self.llm = None
        self.graph_transformer = None
        self.enabled = False
        
        # Only initialize if Neo4j credentials are provided
        if neo4j_uri and neo4j_user and neo4j_password:
            try:
                from langchain_community.graphs import Neo4jGraph
                from langchain_experimental.graph_transformers import LLMGraphTransformer
                
                self.graph = Neo4jGraph(
                    url=neo4j_uri,
                    username=neo4j_user,
                    password=neo4j_password
                )
                self.llm = ChatOpenAI(
                    model="gpt-4",
                    temperature=0,
                    openai_api_key=os.getenv('OPENAI_API_KEY')
                )
                self.graph_transformer = LLMGraphTransformer(llm=self.llm)
                self.enabled = True
                logger.info("✅ Neo4j GraphRAG enabled")
            except Exception as e:
                logger.warning(f"⚠️ Neo4j not available, skipping GraphRAG: {e}")
                self.enabled = False
        else:
            logger.info("ℹ️ Neo4j credentials not provided, skipping GraphRAG")
    
    def add_paper_to_graph(self, paper: Dict) -> None:
        """Add a research paper to the knowledge graph"""
        if not self.enabled:
            return
            
        try:
            # Create paper node
            cypher_query = """
            MERGE (p:Paper {url: $url})
            SET p.title = $title,
                p.abstract = $abstract,
                p.publication_date = $publication_date,
                p.source = $source,
                p.doi = $doi,
                p.score = $score,
                p.updated_at = datetime()
            
            // Add authors
            FOREACH (author_name IN $authors |
                MERGE (a:Author {name: author_name})
                MERGE (a)-[:AUTHORED]->(p)
            )
            
            // Add keywords/topics
            FOREACH (keyword IN $keywords |
                MERGE (k:Keyword {name: keyword})
                MERGE (p)-[:HAS_KEYWORD]->(k)
            )
            """
            
            self.graph.query(cypher_query, {
                'url': paper.get('url', ''),
                'title': paper.get('title', ''),
                'abstract': paper.get('abstract', ''),
                'publication_date': paper.get('publication_date', ''),
                'source': paper.get('source', ''),
                'doi': paper.get('doi', ''),
                'score': paper.get('score', 0.0),
                'authors': paper.get('authors', []),
                'keywords': paper.get('keywords', [])
            })
            
            # Extract entities and relationships from abstract using LLM
            if paper.get('abstract'):
                doc = Document(page_content=paper['abstract'], metadata={
                    'title': paper.get('title', ''),
                    'source': paper.get('source', ''),
                    'url': paper.get('url', '')
                })
                
                graph_docs = self.graph_transformer.convert_to_graph_documents([doc])
                
                # Add extracted entities and relationships
                for graph_doc in graph_docs:
                    for node in graph_doc.nodes:
                        self.graph.query("""
                            MERGE (e:Entity {id: $id})
                            SET e.type = $type,
                                e.properties = $properties
                        """, {
                            'id': node.id,
                            'type': node.type,
                            'properties': node.properties
                        })
                        
                        # Link entity to paper
                        self.graph.query("""
                            MATCH (p:Paper {url: $paper_url})
                            MATCH (e:Entity {id: $entity_id})
                            MERGE (p)-[:MENTIONS]->(e)
                        """, {
                            'paper_url': paper.get('url', ''),
                            'entity_id': node.id
                        })
                    
                    for rel in graph_docs.relationships:
                        self.graph.query("""
                            MATCH (source:Entity {id: $source_id})
                            MATCH (target:Entity {id: $target_id})
                            MERGE (source)-[r:RELATES_TO]->(target)
                            SET r.type = $rel_type,
                                r.properties = $properties
                        """, {
                            'source_id': rel.source.id,
                            'target_id': rel.target.id,
                            'rel_type': rel.type,
                            'properties': rel.properties
                        })
            
            logger.info(f"Added paper to graph: {paper.get('title', '')[:50]}...")
            
        except Exception as e:
            logger.error(f"Error adding paper to graph: {e}")
    
    def add_repo_to_graph(self, repo: Dict) -> None:
        """Add a GitHub repository to the knowledge graph"""
        if not self.enabled:
            return
            
        try:
            cypher_query = """
            MERGE (r:Repository {url: $url})
            SET r.name = $name,
                r.description = $description,
                r.language = $language,
                r.stars = $stars,
                r.last_updated = $last_updated,
                r.updated_at = datetime()
            
            // Add topics
            FOREACH (topic IN $topics |
                MERGE (t:Topic {name: topic})
                MERGE (r)-[:HAS_TOPIC]->(t)
            )
            """
            
            self.graph.query(cypher_query, {
                'url': repo.get('url', ''),
                'name': repo.get('name', ''),
                'description': repo.get('description', ''),
                'language': repo.get('language', ''),
                'stars': repo.get('stars', 0),
                'last_updated': repo.get('last_updated', ''),
                'topics': repo.get('topics', [])
            })
            
            logger.info(f"Added repository to graph: {repo.get('name', '')}")
            
        except Exception as e:
            logger.error(f"Error adding repository to graph: {e}")
    
    def find_related_concepts(self, interests: List[str], limit: int = 20) -> List[Dict]:
        """Find papers and repositories related to user interests"""
        if not self.enabled:
            return {'papers': [], 'repositories': []}
            
        try:
            # Build query for interests
            interest_condition = " OR ".join([f"toLower(k.name) CONTAINS toLower('{interest}')" for interest in interests])
            
            cypher_query = f"""
            MATCH (p:Paper)-[:HAS_KEYWORD]->(k:Keyword)
            WHERE {interest_condition}
            WITH p, COUNT(k) as keyword_matches
            OPTIONAL MATCH (p)-[:MENTIONS]->(e:Entity)
            RETURN p.title as title,
                   p.abstract as abstract,
                   p.url as url,
                   p.source as source,
                   p.publication_date as publication_date,
                   p.score as score,
                   keyword_matches,
                   COLLECT(DISTINCT e.id) as mentioned_entities
            ORDER BY keyword_matches DESC, p.score DESC
            LIMIT {limit}
            """
            
            results = self.graph.query(cypher_query)
            
            # Also find related repositories
            repo_query = f"""
            MATCH (r:Repository)-[:HAS_TOPIC]->(t:Topic)
            WHERE {" OR ".join([f"toLower(t.name) CONTAINS toLower('{interest}')" for interest in interests])}
            WITH r, COUNT(t) as topic_matches
            RETURN r.name as name,
                   r.description as description,
                   r.url as url,
                   r.language as language,
                   r.stars as stars,
                   topic_matches
            ORDER BY topic_matches DESC, r.stars DESC
            LIMIT 10
            """
            
            repo_results = self.graph.query(repo_query)
            
            return {
                'papers': results,
                'repositories': repo_results
            }
            
        except Exception as e:
            logger.error(f"Error finding related concepts: {e}")
            return {'papers': [], 'repositories': []}

class CitationManager:
    """Manages citations and source tracking like Liner"""
    
    def __init__(self):
        self.citations = {}
        self.citation_counter = 1
    
    def add_citation(self, source: Dict, content: str) -> str:
        """Add a citation and return citation marker"""
        citation_id = f"[{self.citation_counter}]"
        self.citations[citation_id] = {
            'title': source.get('title', ''),
            'authors': source.get('authors', []),
            'url': source.get('url', ''),
            'source': source.get('source', ''),
            'publication_date': source.get('publication_date', ''),
            'relevant_content': content[:200] + "..." if len(content) > 200 else content
        }
        self.citation_counter += 1
        return citation_id
    
    def format_citations(self) -> str:
        """Format all citations in Liner style"""
        if not self.citations:
            return ""
        
        formatted = "\n\n## Sources\n"
        for citation_id, info in self.citations.items():
            authors_str = ", ".join(info['authors'][:3]) + ("..." if len(info['authors']) > 3 else "")
            formatted += f"\n{citation_id} **{info['title']}**\n"
            formatted += f"   Authors: {authors_str}\n"
            formatted += f"   Source: {info['source']} ({info['publication_date']})\n"
            formatted += f"   URL: {info['url']}\n"
            formatted += f"   Relevant: {info['relevant_content']}\n"
        
        return formatted

class DummyLLM:
    """Minimal fallback when no OpenAI key is available"""
    def invoke(self, prompt: str, *args, **kwargs):
        return SimpleNamespace(content="Stub response (OpenAI key not set).")

    async def ainvoke(self, prompt: str, *args, **kwargs):
        return SimpleNamespace(content="Stub response (OpenAI key not set).")

class LinerStyleAgent:
    """Liner-style conversational research agent with citations"""
    
    def __init__(self, discovery_agent):
        self.discovery_agent = discovery_agent
        self.citation_manager = CitationManager()
        self.conversation_history = []
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key and api_key.strip():
            self.llm = ChatOpenAI(model="gpt-4", temperature=0.1, openai_api_key=api_key)
        else:
            self.llm = DummyLLM()
    
    async def answer_question(self, question: str, research_context: Dict = None) -> str:
        """Answer questions with citations like Liner"""
        try:
            # If no research context provided, do a quick search
            if not research_context:
                # Extract key terms from question for search
                search_terms = await self._extract_search_terms(question)
                research_context = await self.discovery_agent.discover_research(search_terms, max_per_source=5)
            
            # Reset citations for new question
            self.citation_manager = CitationManager()
            
            # Build context with citations
            context_with_citations = self._build_cited_context(research_context)
            
            # Generate answer with citations
            answer_prompt = f"""
            You are a scientific research assistant like Liner. Answer the user's question using the provided research papers and repositories.

            IMPORTANT: 
            - Cite sources using the provided citation markers [1], [2], etc.
            - Provide specific, accurate information
            - Include line-by-line citations where relevant
            - Be comprehensive but concise
            - Structure your answer clearly

            Question: {question}

            Research Context:
            {context_with_citations}

            Provide a detailed answer with proper citations:
            """
            
            response = await self.llm.ainvoke(answer_prompt)
            answer = response.content
            
            # Add formatted citations
            citations = self.citation_manager.format_citations()
            full_response = answer + citations
            
            # Store in conversation history
            self.conversation_history.append({
                'question': question,
                'answer': full_response,
                'timestamp': datetime.now().isoformat()
            })
            
            return full_response
            
        except Exception as e:
            logger.error(f"Error answering question: {e}")
            return f"I apologize, but I encountered an error while researching your question: {str(e)}"
    
    async def _extract_search_terms(self, question: str) -> List[str]:
        """Extract key search terms from question"""
        try:
            prompt = f"""
            Extract 2-4 key scientific search terms from this question that would be good for searching academic databases:
            
            Question: {question}
            
            Return only the search terms, one per line:
            """
            
            response = await self.llm.ainvoke(prompt)
            terms = [term.strip() for term in response.content.split('\n') if term.strip()]
            return terms[:4]  # Limit to 4 terms
            
        except Exception as e:
            logger.error(f"Error extracting search terms: {e}")
            return [question]  # Fallback to full question
    
    def _build_cited_context(self, research_context: Dict) -> str:
        """Build research context with citation markers"""
        context = ""
        
        # Add papers with citations
        if research_context.get('papers'):
            context += "## Research Papers:\n\n"
            for paper in research_context['papers'][:10]:  # Limit to top 10
                citation_id = self.citation_manager.add_citation(paper, paper.get('abstract', ''))
                context += f"{citation_id} **{paper.get('title', '')}**\n"
                context += f"Abstract: {paper.get('abstract', '')[:300]}...\n"
                context += f"Authors: {', '.join(paper.get('authors', [])[:3])}\n"
                context += f"Source: {paper.get('source', '')} ({paper.get('publication_date', '')})\n\n"
        
        # Add repositories with citations
        if research_context.get('repositories'):
            context += "## Code Repositories:\n\n"
            for repo in research_context['repositories'][:5]:  # Limit to top 5
                citation_id = self.citation_manager.add_citation(repo, repo.get('description', ''))
                context += f"{citation_id} **{repo.get('name', '')}**\n"
                context += f"Description: {repo.get('description', '')}\n"
                context += f"Language: {repo.get('language', '')} | Stars: {repo.get('stars', 0)}\n\n"
        
        return context
    
    async def deep_research(self, query: str) -> str:
        """Perform deep research like Liner's enhanced mode"""
        try:
            # Multi-stage research
            initial_terms = await self._extract_search_terms(query)
            
            # Stage 1: Broad search
            broad_results = await self.discovery_agent.discover_research(initial_terms, max_per_source=15)
            
            # Stage 2: Extract related concepts from initial results
            related_concepts = await self._extract_related_concepts(broad_results)
            
            # Stage 3: Targeted search on related concepts
            targeted_results = await self.discovery_agent.discover_research(related_concepts, max_per_source=10)
            
            # Combine results
            combined_results = {
                'papers': broad_results.get('papers', []) + targeted_results.get('papers', []),
                'repositories': broad_results.get('repositories', []) + targeted_results.get('repositories', []),
                'summary': broad_results.get('summary', '') + '\n\n' + targeted_results.get('summary', '')
            }
            
            # Generate comprehensive answer
            answer = await self.answer_question(query, combined_results)
            
            return answer
            
        except Exception as e:
            logger.error(f"Error in deep research: {e}")
            return await self.answer_question(query)
    
    async def _extract_related_concepts(self, research_results: Dict) -> List[str]:
        """Extract related concepts from research results"""
        try:
            # Combine abstracts and descriptions
            text_content = ""
            for paper in research_results.get('papers', [])[:5]:
                text_content += paper.get('abstract', '') + " "
            
            for repo in research_results.get('repositories', [])[:3]:
                text_content += repo.get('description', '') + " "
            
            prompt = f"""
            From this research content, extract 3-5 related scientific concepts or terms that would be good for further research:
            
            Content: {text_content[:2000]}
            
            Return only the concepts, one per line:
            """
            
            response = await self.llm.ainvoke(prompt)
            concepts = [concept.strip() for concept in response.content.split('\n') if concept.strip()]
            return concepts[:5]
            
        except Exception as e:
            logger.error(f"Error extracting related concepts: {e}")
            return []

class ScientificDiscoveryAgent:
    """Main agent for scientific paper discovery and knowledge graph management"""
    
    def __init__(self):
        self.search_tools = ScientificSearchTools()
        
        # Make Neo4j optional
        neo4j_uri = os.getenv('NEO4J_URI')
        neo4j_user = os.getenv('NEO4J_USERNAME') 
        neo4j_password = os.getenv('NEO4J_PASSWORD')
        
        self.kb_builder = KnowledgeGraphBuilder(
            neo4j_uri=neo4j_uri,
            neo4j_user=neo4j_user,
            neo4j_password=neo4j_password
        )
        
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key and api_key.strip():
            self.llm = ChatOpenAI(model="gpt-4", temperature=0.1, openai_api_key=api_key)
        else:
            logger.warning("OPENAI_API_KEY not set – running ScientificDiscoveryAgent in offline mode")
            self.llm = DummyLLM()
        
        # Initialize Liner-style agent
        self.liner_agent = LinerStyleAgent(self)
        
        # Create ranking and summary functions
        self.rank_papers = self._create_ranking_tool()
        self.summarize_findings = self._create_summary_tool()
        
        # Define enabled sources
        self.enabled_sources = {
            'papers': ['arxiv', 'pubmed', 'bioarxiv'],
            'repos': ['github'],
            'videos': ['youtube']
        }

        # Load environment variables
        _env_path = find_dotenv(usecwd=True) or Path(__file__).resolve().parent.parent / ".env"
        if Path(_env_path).exists():
            load_dotenv(_env_path)
    
    def _create_ranking_tool(self):
        """Create a tool for ranking papers by relevance"""
        def rank_papers_by_relevance(papers: List[Dict], interests: List[str]) -> List[Dict]:
            """Rank papers by relevance to user interests using LLM"""
            if not papers:
                return []
            
            # If no real LLM, fallback immediately
            if isinstance(self.llm, DummyLLM):
                return self._fallback_ranking(papers, interests)
            
            papers_to_rank = papers[:20]  # Limit to 20 papers
            
            # Create simplified paper summaries for ranking
            simplified_papers = []
            for i, paper in enumerate(papers_to_rank):
                simplified = {
                    'id': i,
                    'title': paper.get('title', '')[:200],  # Limit title length
                    'abstract': paper.get('abstract', '')[:300],  # Limit abstract length
                    'source': paper.get('source', ''),
                    'date': paper.get('publication_date', '')
                }
                simplified_papers.append(simplified)
            
            prompt = f"""
            Rank these {len(simplified_papers)} scientific papers by relevance to the research interests: {', '.join(interests)}
            
            Consider:
            1. Title relevance to interests
            2. Abstract content match
            3. Recency (newer papers preferred)
            4. Source quality
            
            Papers:
            {json.dumps(simplified_papers, indent=1)}
            
            Return ONLY a JSON list of paper IDs in order of relevance (most relevant first):
            Example: [2, 0, 5, 1, 3, 4]
            """
            
            try:
                response = self.llm.invoke(prompt)
                content = response.content.strip()
                
                # Try to parse the ranking
                if content.startswith('[') and content.endswith(']'):
                    ranking_ids = json.loads(content)
                    
                    # Reorder papers based on ranking
                    ranked_papers = []
                    for rank, paper_id in enumerate(ranking_ids):
                        if 0 <= paper_id < len(papers_to_rank):
                            paper = papers_to_rank[paper_id].copy()
                            paper['score'] = 100 - (rank * 5)  # Score decreases by 5 for each rank
                            ranked_papers.append(paper)
                    
                    # Add remaining papers with lower scores
                    used_ids = set(ranking_ids)
                    for i, paper in enumerate(papers_to_rank):
                        if i not in used_ids:
                            paper_copy = paper.copy()
                            paper_copy['score'] = max(0, 50 - len(ranked_papers) * 2)
                            ranked_papers.append(paper_copy)
                    
                    # Add any remaining papers that weren't ranked (beyond the 20 limit)
                    for paper in papers[20:]:
                        paper_copy = paper.copy()
                        paper_copy['score'] = 10  # Low score for unranked papers
                        ranked_papers.append(paper_copy)
                    
                    return ranked_papers
                
                else:
                    # Fallback: simple scoring based on keyword matching
                    logger.warning("Could not parse LLM ranking, using fallback scoring")
                    return self._fallback_ranking(papers, interests)
            
            except Exception as e:
                logger.error(f"Error ranking papers: {e}")
                return self._fallback_ranking(papers, interests)
        
        return rank_papers_by_relevance
    
    def _fallback_ranking(self, papers: List[Dict], interests: List[str]) -> List[Dict]:
        """Fallback ranking method using simple keyword matching"""
        for paper in papers:
            score = 0
            title = paper.get('title', '').lower()
            abstract = paper.get('abstract', '').lower()
            
            for interest in interests:
                interest_lower = interest.lower()
                if interest_lower in title:
                    score += 20
                if interest_lower in abstract:
                    score += 10
            
            # Bonus for recent papers
            date_str = paper.get('publication_date', '')
            if date_str:
                try:
                    from datetime import datetime
                    if '2024' in date_str or '2023' in date_str:
                        score += 15
                    elif '2022' in date_str or '2021' in date_str:
                        score += 10
                except:
                    pass
            
            paper['score'] = min(100, score)
        
        return sorted(papers, key=lambda x: x.get('score', 0), reverse=True)
    
    def _create_summary_tool(self):
        """Create a tool for summarizing research findings"""
        def summarize_research_findings(papers: List[Dict], repos: List[Dict]) -> str:
            """Generate a comprehensive summary of research findings"""
            
            prompt = f"""
            You are a scientific analyst. Write a concise, reader-friendly briefing **in Markdown** based on the following inputs.

            Inputs (truncated for brevity):
            Papers JSON: {json.dumps(papers[:5], indent=2)[:1500]}
            Repositories JSON: {json.dumps(repos[:3], indent=2)[:800]}

            Markdown format requirements:
            - Title line beginning with `# ` and a short, catchy headline.
            - Five numbered sections (`## 1. ...` through `## 5. ...`) exactly matching the headings below.
            - Inside each section use bullet lists (`- `) for individual points; keep each bullet ≤ 25 words.

            Sections:
            1. Key Research Trends and Themes
            2. Novel Methodologies or Approaches
            3. Potential Research Gaps
            4. Relevant Code Implementations
            5. Recommendations for Further Investigation
            """
            
            try:
                # No real LLM? return placeholder text
                if isinstance(self.llm, DummyLLM):
                    return "Summary generation disabled (offline mode)."
                response = self.llm.invoke(prompt)
                return response.content
            except Exception as e:
                logger.error(f"Error creating summary: {e}")
                return "Unable to generate summary due to an error."
        
        return summarize_research_findings
    
    def _extract_graph_from_summary(self, summary_text: str) -> Dict[str, Any]:
        """Use LLM to extract concept graph (nodes & edges) from summary."""
        if not summary_text or isinstance(self.llm, DummyLLM):
            return {"nodes": [], "edges": []}

        prompt = f"""
        You are an information extraction assistant. From the summary below, identify up to 15 key scientific concepts (single or short multi-word terms) and important code resources, and the relationships among them. Return STRICTLY the following JSON:

        {{
          "nodes": [{{"id": "string", "title": "string", "type": "concept|method|paper|repo"}} ...],
          "edges": [{{"source": "node_id", "target": "node_id", "type": "relation"}} ...]
        }}

        Use short ids (slugified title). Relations can be "related", "implements", "builds_on", etc.
        Summary:
        """
        prompt += summary_text

        try:
            response = self.llm.invoke(prompt)
            content = response.content.strip()
            import json as _json
            graph = _json.loads(content)
            # basic validation
            if 'nodes' in graph and 'edges' in graph:
                return graph
        except Exception as e:
            logger.warning(f"Graph extraction failed: {e}")
        return {"nodes": [], "edges": []}
    
    async def discover_research(self, interests: List[str], max_per_source: int = 10):
        """Main method to discover research based on interests"""
        try:
            logger.info(f"Starting research discovery for interests: {interests}")
            
            all_papers = []
            all_repos = []
            all_videos = []
            
            # Create a set of tasks to run concurrently
            search_tasks = []
            
            # Search each source
            for interest in interests:
                # Search papers
                if 'arxiv' in self.enabled_sources['papers']:
                    search_tasks.append(asyncio.to_thread(self.search_tools.search_arxiv, interest, max_per_source))
                if 'pubmed' in self.enabled_sources['papers']:
                    search_tasks.append(asyncio.to_thread(self.search_tools.search_pubmed, interest, max_per_source))
                if 'bioarxiv' in self.enabled_sources['papers']:
                    search_tasks.append(asyncio.to_thread(self.search_tools.search_bioarxiv, interest, max_per_source))
                
                # Repo sources
                if 'github' in self.enabled_sources['repos']:
                    search_tasks.append(asyncio.to_thread(self.search_tools.search_github_repos, interest, max_per_source))

                # Video sources
                if 'youtube' in self.enabled_sources['videos']:
                    search_tasks.append(asyncio.to_thread(self.search_tools.search_youtube_transcripts, interest, max_per_source))

            # Run all search tasks
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # Heuristic to determine result type
            for result in search_results:
                # Skip failed tasks or empty lists
                if not isinstance(result, list) or len(result) == 0:
                    continue

                sample = result[0]
                if isinstance(sample, dict):
                    if 'abstract' in sample:
                        all_papers.extend(result)
                    elif 'transcript' in sample:
                        all_videos.extend(result)
                    elif 'stars' in sample:
                        all_repos.extend(result)

            # Remove duplicates
            unique_papers = list({paper['url']: paper for paper in all_papers}.values())
            unique_repos = list({repo['url']: repo for repo in all_repos}.values())
            unique_videos = list({video['id']: video for video in all_videos}.values())
            
            # After searching, we may not have results, so handle this
            if not all_papers:
                logger.info("No papers found after searching all sources.")

            # Rank papers by relevance using LLM
            ranked_papers = await self._rank_results(all_papers, interests) if all_papers else []
            logger.info(f"Ranked {len(ranked_papers)} papers.")

            # Search GitHub for repositories
            all_repos = []
            if 'github' in self.enabled_sources['repos']:
                for interest in interests:
                    try:
                        interest_repos = self.search_tools.search_github_repos(interest, max_per_source)
                        all_repos.extend(interest_repos)
                    except Exception as e:
                        logger.error(f"Error searching GitHub for '{interest}': {e}")
            
            # Search YouTube for videos
            all_videos = []
            if 'youtube' in self.enabled_sources.get('videos', []):
                for interest in interests:
                    try:
                        interest_videos = self.search_tools.search_youtube_transcripts(interest, max_per_source)
                        all_videos.extend(interest_videos)
                    except Exception as e:
                        logger.error(f"Error searching YouTube for '{interest}': {e}")

            # Generate synthesis summary (only if real LLM)
            summary_text = ""
            try:
                summary_text = self.summarize_findings(ranked_papers[:10], all_repos[:5])
                graph_data = self._extract_graph_from_summary(summary_text)
            except Exception as e:
                logger.warning(f"Summary generation failed: {e}")
                graph_data = {"nodes": [], "edges": []}

            # YouTube RAG QA synthesis
            youtube_qa = ""
            try:
                youtube_qa = await ask_over_topic(" ".join(interests[:2]),
                                                 f"What are the main video insights about {'; '.join(interests)}?")
            except Exception as e:
                logger.warning(f"YouTube RAG failed: {e}")

            logger.info(f"Discovery complete: {len(ranked_papers)} papers, {len(all_repos)} repos, {len(all_videos)} videos")

            return {
                "papers": ranked_papers, 
                "repositories": all_repos,
                "videos": all_videos,
                "youtube_qa": youtube_qa,
                "summary": summary_text,
                "graph": graph_data
            }
            
        except Exception as e:
            logger.error(f"Error in research discovery: {e}", exc_info=True)
            return {
                'papers': [],
                'repositories': [],
                'videos': [],
                'summary': 'Error occurred during research discovery',
                'related_concepts': {'papers': [], 'repositories': []},
                'total_papers': 0,
                'total_repositories': 0,
                'total_videos': 0
            }

    async def chat(self, message: str, mode: str = "standard") -> str:
        """Chat interface like Liner"""
        if mode == "deep":
            return await self.liner_agent.deep_research(message)
        else:
            return await self.liner_agent.answer_question(message)

    async def _rank_results(self, results: List[Dict], interests: List[str]) -> List[Dict]:
        """Rank papers by relevance using LLM"""
        try:
            # rank_papers is sync; run in thread to avoid blocking and to allow await
            ranked_papers = await asyncio.to_thread(self.rank_papers, results, interests)
            logger.info(f"Ranked {len(ranked_papers)} papers.")
            return ranked_papers
        except Exception as e:
            logger.error(f"Error ranking results: {e}")
            return results

def rate_limit(calls_per_second=1):
    min_interval = 1.0 / calls_per_second
    last_called = [0.0]
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        return wrapper
    return decorator

# Example usage
if __name__ == "__main__":
    # Initialize agent
    agent = ScientificDiscoveryAgent()
    
    # Example research interests
    interests = ["neoantigen", "mhc", "liquid biopsy", "cfDNA"]
    
    # Discover research
    results = asyncio.run(agent.discover_research(interests))
    
    print(f"\nFound {len(results['papers'])} papers, {len(results['repositories'])} repositories, and {len(results['videos'])} videos")
    print(f"\nResearch Summary:\n{results['summary']}")
    
    # Interactive chat mode
    print("\n" + "="*50)
    print("Scientific Discovery Agent - Chat Mode")
    print("Type 'quit' to exit")
    print("="*50)
    
    while True:
        user_input = input("\nYou: ")
        if user_input.lower() in ['quit', 'exit']:
            break
        
        response = agent.chat(user_input)
        print(f"\nAgent: {response}") 